{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3656, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dla bazy danych ver2\n",
    "df = pd.read_csv(\"data/ver2.csv\")\n",
    "print(df.shape)\n",
    "df.head()\n",
    "target = \"TenYearCHD\"\n",
    "numFeatures = ['age', 'log_totChol','log_sysBP', 'log_BMI', 'heartRate', 'log_glucose']\n",
    "catFeatures = [ 'male', 'education', 'currentSmoker', 'prevalentHyp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "mapy = {}\n",
    "for feature in catFeatures:\n",
    "    # Iniciujemy obiekt do kodowania, który będzie przechowywał naszą mapę\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    # Dopasowujemy danę (tworzymy mapę) i od razu ją nakadamy na zmienną na której robiliśmy dopasowanie\n",
    "    df[feature] = le.fit_transform(df[feature])\n",
    "\n",
    "    # Zapiszmy nasza mapę, aby móc odzyskać oryginalne dane bez problemu\n",
    "    mapy[feature] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'age', 'education', 'currentSmoker', 'prevalentHyp', 'heartRate', 'log_glucose', 'log_BMI', 'log_sysBP', 'log_totChol']\n"
     ]
    }
   ],
   "source": [
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(train, validate, features, target, # argumenty obowiązkowe\n",
    "            eta=0.03, max_depth=7, subsample = 0.7, colsample_bytree = 0.7, # hiperparametry\n",
    "            colsample_bylevel=1,lambdaX = 1, alpha=0, gamma=0, min_child_weight=0, # hiperparametry\n",
    "            rate_drop = 0.2, skip_drop=0.5, # hiperparametry\n",
    "            num_boost_round = 1000, early_stopping_rounds = 50, # hiperparametry\n",
    "            debug=True, eval_metric= [\"auc\"], objective = \"binary:logistic\", # konfiguracja\n",
    "            seed=2017, booster = \"gbtree\", tree_method=\"hist\", grow_policy=\"depthwise\",\n",
    "           verbosity=1, silent=False): # konfiguracja\n",
    "    '''\n",
    "    Uniwersalny wrapper dla XGB dla gbtree i dart\n",
    "    Więcej opcji https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    \n",
    "    Parametry\n",
    "    ----------\n",
    "    train, validate, features, target : wymagane zmienne bez domyślnych wartości\n",
    "    train, validate : pd.DataFrames z kolumnami opisanymi w features i target\n",
    "    features : lista zmiennych do wykorzystania w trenowaniu\n",
    "    target : nazwa zmiennej objasnianej\n",
    "    \n",
    "    --- Zmienne wspólne dla gbtree i dart\n",
    "        --- Zmienne właściwe dla Ensamble/Boosting\n",
    "        eta : \"learning rate\"\n",
    "        max_depth=7 : maksymalna głębokość drzew [0,∞]\n",
    "        subsample : udział (0,1] obserwacji do treningu jednej iteracji\n",
    "        colsample_bytree : udział (0,1] kolumn do treningu jednej iteracji\n",
    "        colsample_bylevel : udział  (0,1] kolumn na poziom do treningu jednej iteracji\n",
    "        --- Zmienne regularyzacyjne\n",
    "        lambdaX=0 : regularyzacja L2 [0,∞]\n",
    "        alpha=0 : regularyzacja L1 [0,∞]\n",
    "        gamma=1 : minimalna redukcja funkcji straty\n",
    "        min_child_weight=0 : minimalna suma wg poddrzewa\n",
    "\n",
    "    --- Zmienne dla algorytmu dart\n",
    "    rate_drop : \n",
    "    skip_drop : \n",
    "    \n",
    "    --- Zmienne dla XGB, opis/agorytm/liczba drzew etc.\n",
    "    num_boost_round : maksymalna liczba iteracji\n",
    "    early_stopping_rounds : margines iteracji dla early stopping\n",
    "    debug : Czy włączyć pełne opisy.\n",
    "    eval_metric : Pełna lista dostępna https://github.com/dmlc/xgboost/blob/master/doc/parameter.md \n",
    "    objective : reg:linear, reg:logistic, binary:logistic, multi:softmax lub inne Pełna lista dostępna https://github.com/dmlc/xgboost/blob/master/doc/parameter.md \n",
    "    seed : random seed\n",
    "    booster : ‘auto’, ‘exact’, ‘approx’, ‘hist’, ‘gpu_exact’, ‘gpu_hist’- silnik dla drzew gbtree (cart), dart (gbtree z dropoutem) lub gblinear\n",
    "    tree_method : zobacz http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "    grow_policy : depthwise, lossguide\n",
    "    '''\n",
    "   \n",
    "    start_time = time.time()\n",
    "    paramList= ['eta', 'max_depth',\n",
    "         'subsample', 'colsample_bytree', 'colsample_bylevel',\n",
    "         'lambdaX', 'alpha', 'gamma', 'min_child_weight',\n",
    "         'num_boost_round', 'early_stopping_rounds',\n",
    "         'rate_drop', 'skip_drop',\n",
    "         'eval_metric', 'objective', \n",
    "          'seed', 'booster', 'tree_method', 'grow_policy', 'verbosity', 'silent']\n",
    "    \n",
    "    # Stworzenie słownika do przekazania do XGB\n",
    "    params = dict()\n",
    "    for param in paramList:\n",
    "        params[param]=eval(param)\n",
    "    if(debug):\n",
    "        for param in paramList:\n",
    "            print(param, eval(param), end=\", \")\n",
    "        print('\\nLength train:', len(train.index))\n",
    "        print('Length valid:', len(validate.index))\n",
    "\n",
    "    # Automatyczne przekazanie liczby klas dla wielu poziomów klasyfikacji\n",
    "    if (params[\"objective\"]==\"multi:softmax\" or params[\"objective\"]==\"multi:softprob\"):\n",
    "        params[\"num_class\"]=train[target].nunique()\n",
    "    params[\"silent\"]=1\n",
    "    \n",
    "    # XGB wymaga w słowniku parametrów słowa kluczowego lambda, poniżej workaround\n",
    "    params[\"lambda\"]=lambdaX\n",
    "    \n",
    "    # Przekształcenie zbiorów do struktury DMatrix\n",
    "    # Struktura danych DMatrix pozwala na efektywne tworzenie drzew\n",
    "    dtrain = xgb.DMatrix(train[features].values, train[target].values, feature_names=train[features].columns.values)\n",
    "    dvalid = xgb.DMatrix(validate[features].values, validate[target].values, feature_names=validate[features].columns.values)\n",
    "\n",
    "    # Stworzenie listy zbiorów do ealuacji\n",
    "    evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "    \n",
    "    # Stworzenie zmiennej słownikowej do przekazania\n",
    "    train_history = dict()\n",
    "\n",
    "    # Uruchomienie algorytmu trenującego\n",
    "    gbm = xgb.train(params, dtrain, \n",
    "                    num_boost_round, early_stopping_rounds=early_stopping_rounds,\n",
    "                    evals=evals, evals_result=train_history, verbose_eval=debug)\n",
    "    \n",
    "    # Obliczenai statystyk i dodatkowych wartości\n",
    "    score = gbm.best_score\n",
    "\n",
    "    # Przekształcenie historii trenowania do Pandas Data Frame\n",
    "    trainHistory = dict()\n",
    "    for key in train_history.keys():\n",
    "        for metric in train_history[key].keys():\n",
    "            trainHistory[key+metric.upper()]=train_history[key][metric]\n",
    "    train_history = pd.DataFrame(trainHistory)\n",
    "\n",
    "    # Zapis przewidywanych wartości dla zbioru walidacyjnego dla najlepszej (z punktu widzenia walidacji) iteracji\n",
    "    trainPred = gbm.predict(dtrain, ntree_limit=gbm.best_iteration)\n",
    "    testPred = gbm.predict(dvalid, ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    # Przygotowanie posortowanej listy dla wazności zmiennych, zamaist słownika\n",
    "    imp = gbm.get_fscore()\n",
    "    imp = sorted(imp.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    impFig=None\n",
    "    if(debug):\n",
    "        print('Czas trenowania: {} minut'.format(round((time.time() - start_time)/60, 2)))\n",
    "        # Skorzystanie z wbudowanej funkcji wizualizującej waznośc zmiennych\n",
    "        impFig, ax = plt.subplots()\n",
    "        xgb.plot_importance(gbm, ax=ax)\n",
    "    return score, trainPred, testPred, train_history, impFig, imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Walidacja krzyżowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Przygotujmy prosty wrapper\n",
    "def CVTestXGB(nFolds = 5, randomState=2020, features=features, debug=False, *args, **kwargs):\n",
    "    \n",
    "    # Przygotujmy walidację krzyżową\n",
    "    kf = KFold(n_splits=nFolds, shuffle=True, random_state=randomState)\n",
    "\n",
    "    # Zapisanie wyników ze zbioru treningowego i testowego\n",
    "    testResults = []\n",
    "    trainResults = []\n",
    "    \n",
    "    # Przechowajmy również predykcje dla poszczególnych foldów\n",
    "    predictions = []\n",
    "    \n",
    "    # Razem z informacją o tym, jaki był ich indeks w oryginalnym zbiorze danych\n",
    "    indices = []\n",
    "    \n",
    "    # Przygotujmy listę historii\n",
    "    hists = []\n",
    "    fold = 1\n",
    "    for train, test in kf.split(df.index.values):\n",
    "        # Przygotowanie estymatora\n",
    "        score, trainPred, testPred, train_history, impFig, imp = run_xgb(df.iloc[train], df.iloc[test],\n",
    "                                                                         features, target, debug=debug, *args, **kwargs)\n",
    "\n",
    "        # Zachowajmy informacje o predykcjach dla tego foldu\n",
    "        predictions.append(testPred.tolist().copy())\n",
    "        \n",
    "        # Razem z indeksami w oryginalnym data frame\n",
    "        indices.append(df.iloc[test].index.tolist().copy())\n",
    "        \n",
    "        # Informowanie o każdym foldzie razem z wynikami treningowymi możemy opcjonalnie wyświetlać w trakcie\n",
    "        trainScore = roc_auc_score(df[target].iloc[train], trainPred)\n",
    "        testScore = roc_auc_score(df[target].iloc[test], testPred)\n",
    "        \n",
    "        trainResults.append(trainScore)\n",
    "        testResults.append(testScore)\n",
    "        \n",
    "        hists.append(train_history.add_suffix('_'+str(fold)))\n",
    "        fold+=1\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Train AUC:\", trainScore,\n",
    "                  \"Valid AUC:\", testScore)\n",
    "        \n",
    "    return trainResults, testResults, predictions, indices, pd.concat(hists, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:44:30] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:44:30] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:44:31] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:44:32] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:44:34] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.7132287843303132 (8, 0.5689260909353719, 0.18100274811537684, 3.770176460589245, 8.518970075080587)\n",
      "[04:44:34] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "results=[]\n",
    "paramList = []\n",
    "for x in range(100):\n",
    "    params = (random.randint(3, 9), random.uniform(0.1, 0.9), random.uniform(0.1, 0.9), random.uniform(0, 10), random.uniform(0, 10))\n",
    "    trainResults, testResults, predictions, indices, hists = CVTestXGB(\n",
    "        max_depth=params[0],\n",
    "        subsample = params[1],\n",
    "        colsample_bytree = params[2],\n",
    "        lambdaX=params[3],\n",
    "        gamma=params[4])\n",
    "    print(np.mean(testResults), params)\n",
    "    results.append(np.mean(testResults))\n",
    "    paramList.append(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7285448242146847, (9, 0.2991584259346361, 0.8112936001356827, 6.995409550001176, 3.844416103106021))\n",
      "(0.727552513136642, (8, 0.27755492366907075, 0.734986578709555, 9.32643523611403, 1.167332095076522))\n",
      "(0.7274551328952794, (3, 0.4303771229225519, 0.750119310162075, 4.697861833593212, 0.20497264055284403))\n",
      "(0.7274539741238252, (9, 0.30719961142347796, 0.5003275523838066, 6.782874505432547, 1.5094976234865776))\n",
      "(0.7272683598483443, (6, 0.2485855427774534, 0.6409545658164972, 5.897133948449324, 0.520659877221149))\n",
      "(0.7271443360482092, (8, 0.8192873897273915, 0.6388911829601795, 8.889678472737884, 2.2834621912321564))\n",
      "(0.7271295862001609, (6, 0.2680123136108479, 0.5122529290796449, 8.301383213378642, 2.6147902649789225))\n",
      "(0.7269818135926615, (5, 0.5670092306874904, 0.8882394807055124, 2.8348119976517916, 1.4649131816662764))\n",
      "(0.7267386118397636, (8, 0.5298643207860769, 0.7332850198219332, 8.235566379610946, 4.199047551075293))\n",
      "(0.7266004001759372, (6, 0.7079555025379549, 0.8706585174951512, 7.538672784323101, 1.6420892451303448))\n"
     ]
    }
   ],
   "source": [
    "maxRes = []\n",
    "for i in range(1, len(paramList)):\n",
    "    maxRes.append(max(results[0:i]))\n",
    "imp = list(zip(results, paramList))\n",
    "imp.sort(reverse=True)\n",
    "for row in imp[0:10]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(len(results)-1)), maxRes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:37:53] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:37:54] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:37:54] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:37:55] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:37:56] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, lambdaX, num_boost_round, rate_drop, silent, skip_drop } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.7285448242146847\n"
     ]
    }
   ],
   "source": [
    "trainResults, testResults, predictions, indices, hists = CVTestXGB(\n",
    "        max_depth=9, subsample = 0.2991584259346361, colsample_bytree = 0.8112936001356827, lambdaX=6.995409550001176, gamma=3.844416103106021)\n",
    "print(np.mean(testResults))\n",
    "\n",
    "modelXGB_rs = {\n",
    "    \"name\":\"XGB_rs\",\n",
    "    \"description\":\"Model XGB, ze zmiennymi kategorycznymi z LE\",\n",
    "    \"specification\":'max_depth=9, subsample = 0.2991584259346361, colsample_bytree = 0.8112936001356827, lambdaX=6.995409550001176, gamma=3.844416103106021',\n",
    "    \"trainResults\":trainResults.copy(),\n",
    "    \"testResults\":testResults.copy(),\n",
    "    \"predictions\":predictions.copy(),\n",
    "    \"indices\":indices.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otwieramy plik do zapisu binarnego z wykorzystenim with\n",
    "with open(\"model_XGB_1_ver2_rs.p\", \"wb\") as fp:\n",
    "    # Zapisujemy obiekt do wskaźnika pliku\n",
    "    pickle.dump(modelXGB_rs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
